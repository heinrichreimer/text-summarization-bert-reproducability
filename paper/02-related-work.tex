\section{Related Work} % 80/300 points, 27%

In the reproduced article, \citeauthor{LiuL2019} use the encoder from the pretrained \Bert language model to build absractive text summarization model~\cite{LiuL2019,DevlinCLT2019}.
While neural architectures have previously been used for abstractive summarization~\cite{NallapatiZSGX2016,SeeLM2017,Paulus2018}, it has been difficult to generate fluent yet informative summaries because those models can only model linguistics from limited training data~\todocite. Common benchmark datasets like the CNN/Daily Mail datasets for supervised learning contain far less data then the amount available for unsupervised pretraining of language models~\cite{HermannKGEKSB2015,DevlinCLT2019}.
By fine-tuning a transformer encoder-decoder architecture with pretrained encoders~\cite{VaswaniSPUJGKP2017,DevlinCLT2019}, \citeauthor{LiuL2019} leverage broader linguistic knowledge from the pretrained language model for generating better summaries~\cite{LiuL2019}.
As predicted in their article, now \todo{larger} language models are able to generate even better abstractive summaries~\todocite\cite{LiuL2019}.

\subsection{Abstractive Summarization}

\todo{What is abstractive summarization?}

% Reproduced approach is based on:
\citeauthor{Sutskever2014?,Chopra2016?,NallapatiZSGX2016}: neural encoder-decoder network as RNN.
\citeauthor{NallapatiZSGX2016}: sequence-to-sequence modelling, first to use CNN/Daily Mail~\cite{HermannKGEKSB2015} for evaluating summarization quality~\cite{T5}, generator/pointer setting.
\citeauthor{???}: abstractive summarization is a generative task similar to neural machine translation.

% Other approaches:
\citeauthor{Paulus2018,SeeLM2017}: generative models sometimes produce unnatural, repetitive summaries.
\citeauthor{Paulus2018}: attention over input and generated output, take into account what has already been generated (\Bert does that too with output transformers~\cite{DevlinCLT2019}?), uses teacher forcing.
\citeauthor{SeeLM2017}: hybrid pointer-generator network with coverage to avoid repetition and factual errors.

\subsection{Summarization With Pretrained Language Models}

% Reproduced approach is based on:
\citeauthor{DevlinCLT2019}: pretrained language model, based on multi-head attention~\cite{VaswaniSPUJGKP2017}, trained on a large corpus (3300M tokens), \Bert parameters jointly fine-tuned with task-specific parameters.
\citeauthor{Edunov2019?,Rothe2019?}: use \Bert for generative tasks.

% Reproduced approach:
\citeauthor{LiuL2019}: abstractive summarization as encoder-decoder generative approach, using pretrained \Bert encoder~\cite{DevlinCLT2019}, transformer decoder~\cite{VaswaniSPUJGKP2017}, custom trained position embeddings.
Open details not covered completely in the replicated article.
Which details can be explained and added by our reproduction.
Explain those particular details.
Which principles, ideas, or assumptions are used or made in the replicated approach?

% Other approaches:
\citeauthor{???}: other (larger) pretrained models used for abstractive summarization.
BART, BART+R3F, ERNIE-GENLARGE?

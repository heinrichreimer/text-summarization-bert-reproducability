\documentclass[english,acmlarge,nonacm,natbib=false,urlbreakonhyphens=false,screen]{acmart}
\usepackage{babel}
\usepackage{csquotes}
\usepackage{listings}
\usepackage[style=alphabetic]{biblatex}

\title{Reproducing Text Summarization with Pretrained Encoders}
\author{Jan Heinrich Reimer}
\orcid{0000-0003-1992-8696}
\affiliation{
    \institution{Martin Luther University Halle-Wittenberg}
    % \department{Institute for Computer Science}
    \streetaddress{Von-Seckendorff-Platz~1}
    \postcode{06108}
    \city{Halle (Saale)}
    % \state{Sachsen-Anhalt}
    \country{Germany}
}
\email{jan.reimer@student.uni-halle.de}

% Don't display a book title or conference.
\acmBooktitle{}
\acmConference{}{}{}

\addbibresource{../literature/literature.bib}
\nocite{*} % TODO Remove before publishing.

\begin{document}

\begin{abstract}
    Pretrained encoders like BERT are commonly used for summarizing text. \citeauthor{LiuL2019} introduce models for extractive and abstractive summarization based on the pretrained BERT model~\cite{LiuL2019}.
    We reproduce their models and experiments on the Flux~\cite{InnesSFGRJKPS2018} framework.
\end{abstract}

\maketitle

\input{sections/01-introduction.tex}
\input{sections/02-related-work.tex}
\input{sections/03-fine-tuning-summarization-encoder.tex}
\input{sections/04-experimental-evaluation.tex}
\input{sections/05-conclusion-future-work.tex}

\printbibliography

\end{document}
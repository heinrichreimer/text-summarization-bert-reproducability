\documentclass[english,sigconf,nonacm,screen,review]{acmart}
\usepackage{babel}
\usepackage{hyperref}

\title{Reproducing Text Summarization with Pretrained Encoders}
\author{Jan Heinrich Reimer}
\orcid{}
\affiliation{
    \institution{Martin Luther University Halle-Wittenberg}
    % \department{Institute for Computer Science}
    \streetaddress{Von-Seckendorff-Platz~1}
    \postcode{06108}
    \city{Halle (Saale)}
    % \state{Sachsen-Anhalt}
    \country{Germany}
}
\email{jan.reimer@student.uni-halle.de}

\begin{document}

\begin{abstract}
    Pretrained encoders like BERT are commonly used for summarizing text. Liu and Lapata introduce models for extractive and abstractive summarization based on the pretrained BERT model.
    We reproduce their models and experiments on the \href{https://fluxml.ai/}{Flux} framework.
\end{abstract}

\maketitle

\end{document}
Large pretrained transformer encoders like \Bert~\cite{DevlinCLT2019} have successfully been used for various tasks from the field of natural language processing~\cite{LiuL2019}. \citeauthor{LiuL2019} build upon the pretrained \Bert model to tackle extractive and abstractive text summarization~\cite{LiuL2019}. Their abstractive summarization outmatches many previously state-of-the-art summarizers in informativeness and fluency measured using \Rouge metrics~\cite{LiuL2019,Lin2004}.
Since their publication, other fine-tuning models were introduced that surpass their performance~\cite{AghajanyanSGGZG2020,SavelievaAR2020}.
In this report, we reproduce the abstractive summarization experiments of~\citeauthor{LiuL2019} using the Flux~\cite{InnesSFGRJKPS2018} framework by re-implementing their \BertSumAbs model.
We highlight differences and difficulties in experimental replication to build a better understanding of abstractive text summarization using pretrained encoders. 
